# Analyseur de Manuel Utilisateur ‚Äî PoC NLP

Proof of Concept **TypeScript / Node.js** pour analyser des manuels utilisateur
(PDF ou texte), extraire des informations structur√©es via NLP, et produire un
JSON exploitable par un LLM.

## Architecture

```
analyser-ia/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ analyser.ts               # Point d'entr√©e ‚Äî orchestration du pipeline
‚îÇ   ‚îú‚îÄ‚îÄ types.ts                  # Interfaces TypeScript partag√©es
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ       ‚îú‚îÄ‚îÄ textExtractor.ts      # Extraction texte depuis .pdf ou .txt
‚îÇ       ‚îú‚îÄ‚îÄ sectionParser.ts      # D√©coupage du texte en sections
‚îÇ       ‚îú‚îÄ‚îÄ nlpProcessor.ts       # Analyse NLP (entit√©s, actions, r√©sum√©)
‚îÇ       ‚îî‚îÄ‚îÄ embedder.ts           # (Optionnel) G√©n√©ration d'embeddings
‚îú‚îÄ‚îÄ dist/                         # Sortie compil√©e (tsc)
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ package.json
```

## Pr√©requis

- Node.js >= 18
- pnpm (ou npm / yarn)

## Installation

```bash
pnpm install

docker run -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=postrgres -e POSTGRES_DB=ask_rules --name postgres_vector -p 5432:5432 -d ankane/pgvector
```

## Utilisation

### D√©veloppement ‚Äî ex√©cution directe avec ts-node

```bash
# Analyser un fichier texte
npx ts-node src/analyser.ts data/manuel.txt

# Analyser un fichier PDF
npx ts-node src/analyser.ts data/manuel.pdf

# Avec embeddings Transformers.js local (recommand√©, multilingue, offline)
pnpm add @huggingface/transformers  # Installation unique
npx ts-node src/analyser.ts data/manuel.txt --embed
# Au 1er lancement : t√©l√©charge le mod√®le (~50MB), puis utilise le cache

# Avec embeddings OpenAI (optionnel)
OPENAI_API_KEY=sk-... npx ts-node src/analyser.ts data/manuel.txt --embed

# Fichier de sortie personnalis√©
npx ts-node src/analyser.ts data/manuel.txt --output exports/mon_analyse.json
```

### Production ‚Äî compilation puis ex√©cution

```bash
pnpm run build                         # Compile src/ ‚Üí dist/
node dist/analyser.js data/manuel.txt
```

### Via les scripts npm

```bash
pnpm start              # ts-node data/manuel.txt ‚Üí data/resultat.json
pnpm run analyse:txt    # Identique
pnpm run analyse:pdf    # Analyse data/manuel.pdf
pnpm run build          # Compile TypeScript vers dist/
pnpm run start:dist     # node dist/ (apr√®s build)
```

## Modes d'embeddings

Le module `embedder.ts` supporte **3 modes** avec s√©lection automatique :

### Mode 1 : Transformers.js local (‚úÖ Recommand√©)

```bash
pnpm add @huggingface/transformers  # Installation unique
npx ts-node src/analyser.ts data/manuel.txt --embed
```

- **Mod√®le** : `Xenova/multilingual-e5-small` (384 dims)
- **Avantages** :
  - ‚úÖ 100% gratuit et offline (apr√®s 1er t√©l√©chargement)
  - ‚úÖ Multilingue optimis√© (fran√ßais, anglais, etc.)
  - ‚úÖ Pas de cl√© API requise
  - ‚úÖ Ex√©cution locale en Node.js
- **1er lancement** : T√©l√©charge automatiquement le mod√®le (~50MB), puis cache local

### Mode 2 : OpenAI (optionnel)

```bash
export OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxx
pnpm add openai  # Installation requise
npx ts-node src/analyser.ts data/manuel.txt --embed
```

- **Mod√®le** : `text-embedding-3-small` (1536 dims)
- **Avantages** : Tr√®s performant, multilingue
- **Inconv√©nients** : Payant, n√©cessite une connexion internet

### Mode 3 : TF-IDF local (fallback)

```bash
# Automatique si aucun embedding disponible
npx ts-node src/analyser.ts data/manuel.txt --embed
```

- **Avantages** : Aucune d√©pendance externe, ultra-l√©ger
- **Inconv√©nients** : Qualit√© inf√©rieure aux embeddings neuronaux

**Ordre de priorit√©** : Transformers.js ‚Üí OpenAI ‚Üí TF-IDF.

## Cache Redis

Pour r√©duire les co√ªts d'API et am√©liorer les performances, un syst√®me de cache Redis a √©t√© int√©gr√© :

### Configuration

```bash
# 1. Lancer Redis (Docker)
docker run -d -p 6379:6379 --name ask-rules-redis redis:7-alpine

# 2. Configurer l'application (.env)
REDIS_URL=redis://localhost:6379

# 3. Installer les d√©pendances
pnpm install
```

### Fonctionnement

- ‚úÖ **Cache automatique** : Les questions/r√©ponses sont mises en cache pendant 24h
- ‚úÖ **Normalisation** : Les questions similaires utilisent le m√™me cache
- ‚úÖ **Mode graceful** : Si Redis n'est pas disponible, l'app continue sans cache
- ‚ö° **Performance** : ~50-200ms depuis le cache vs 1-5s depuis le LLM

### Documentation compl√®te

Consultez [docs/REDIS_CACHE_GUIDE.md](docs/REDIS_CACHE_GUIDE.md) pour :

- Configuration d√©taill√©e
- Services Redis manag√©s (AWS, Redis Cloud, etc.)
- Commandes de monitoring
- D√©pannage

## Protection Anti-Spam

Un syst√®me de rate limiting prot√®ge l'application contre les abus :

### Fonctionnement

- üõ°Ô∏è **Limite** : Maximum 10 questions par minute par IP
- ‚è±Ô∏è **Blocage** : 5 minutes apr√®s d√©passement
- üéØ **D√©tection IP** : Support des proxies (X-Forwarded-For, X-Real-IP)
- ‚úÖ **Whitelist** : IPs exempt√©es configurables

### Configuration

```env
# Optionnel : IPs exempt√©es du rate limiting
RATE_LIMIT_WHITELIST=127.0.0.1,::1
```

### Documentation compl√®te

Consultez [docs/RATE_LIMITING_GUIDE.md](docs/RATE_LIMITING_GUIDE.md) pour :

- Personnalisation des limites
- Monitoring et statistiques
- D√©blocage manuel d'IPs
- Tests et s√©curit√© en production

## Format de sortie (resultat.json)

```json
{
  "manuel": "manuel",
  "fichier": "/chemin/absolu/data/manuel.txt",
  "date_analyse": "2026-02-20T10:00:00.000Z",
  "statistiques": {
    "caracteres": 7829,
    "mots": 974,
    "sections": 22,
    "entites_total": 44,
    "actions_total": 89
  },
  "sections": [
    {
      "titre": "INSTALLATION",
      "contenu": "Follow the steps below to install SmartFlow Pro...",
      "entites": ["configure", "download", "engine", "finish"],
      "actions": ["accept", "click", "choose", "complete", "download"],
      "resume": "Follow the steps below to install SmartFlow Pro on your machine.",
      "embedding": { "install": 0.42, "wizard": 0.31, "database": 0.28 }
    }
  ]
}
```

## Biblioth√®ques utilis√©es

| Biblioth√®que | Usage                                           | Licence    |
| ------------ | ----------------------------------------------- | ---------- |
| `compromise` | NLP : extraction d'entit√©s et de verbes         | MIT        |
| `pdfreader`  | Extraction de texte depuis des PDFs             | Apache 2.0 |
| `chalk`      | Affichage color√© dans le terminal               | MIT        |
| `typescript` | Compilation et typage statique                  | Apache 2.0 |
| `ts-node`    | Ex√©cution TypeScript sans compilation pr√©alable | MIT        |

## Interfaces TypeScript (src/types.ts)

```typescript
interface Section {
  titre: string;
  contenu: string;
  entites: string[];
  actions: string[];
  resume: string;
  embedding?: number[] | Record<string, number> | null;
}

interface AnalysisResult {
  manuel: string;
  fichier: string;
  date_analyse: string;
  statistiques: Statistics;
  sections: Section[];
}
```

## Support du fran√ßais

`compromise` est optimis√© pour l'anglais. Pour analyser des textes en fran√ßais :

**Option 1 ‚Äî Plugin compromise-fr**

```bash
pnpm add compromise-fr
```

```typescript
import nlp from 'compromise';
import frPlugin from 'compromise-fr';
nlp.extend(frPlugin);
```

**Option 2 ‚Äî nlp.js (support multilingue natif)**

```bash
pnpm add node-nlp @nlpjs/lang-fr
```

Voir [documentation nlp.js](https://github.com/axa-group/nlp.js)

## Extension : base vectorielle

Pour indexer les embeddings dans une base vectorielle et interroger le manuel
depuis un LLM, vous pouvez utiliser :

- **[ChromaDB](https://www.trychroma.com/)** ‚Äî base vectorielle locale, SDK JS disponible
- **[pgvector](https://github.com/pgvector/pgvector)** ‚Äî extension PostgreSQL
- **[Weaviate](https://weaviate.io/)** ‚Äî base vectorielle cloud/local

Exemple d'indexation avec Chroma :

```typescript
import { ChromaClient } from 'chromadb';
import type { AnalysisResult } from './src/types';

const client = new ChromaClient();
const collection = await client.createCollection({ name: 'manuel' });
const resultat: AnalysisResult = JSON.parse(fs.readFileSync('data/resultat.json', 'utf-8'));

for (const section of resultat.sections) {
  await collection.add({
    ids: [section.titre],
    embeddings: [section.embedding as number[]], // vecteur OpenAI dense
    documents: [section.contenu],
    metadatas: [{ actions: section.actions.join(',') }],
  });
}
```
